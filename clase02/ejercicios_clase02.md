
# Clase 2 ‚Äî Resumen Enriquecido de Ejercicios (desde *Evaluaci√≥n de Residuos*)

Este documento sintetiza y enriquece los puntos trabajados en el notebook de la Clase 2,
a partir de la secci√≥n **1. Evaluaci√≥n de Residuos**. Incluye consignas, notas catalogadas
como *IMPORTANTE*, conclusiones de bloques de c√≥digo y aportes adicionales.

---

## 1. Evaluaci√≥n de Residuos
- **Qu√© se hace:** Se grafican residuos vs predicciones (`LassoCV`).
- **Conclusi√≥n del c√≥digo:**
  - Los residuos se distribuyen alrededor de 0, de manera aleatoria.
  - No hay patr√≥n evidente ‚Üí el modelo est√° capturando bien la relaci√≥n lineal.
  - Los errores no dependen del valor predicho ‚Üí condici√≥n deseable en un modelo lineal.
- **Idea clave:** Una buena regresi√≥n lineal debe mostrar residuos aleatorios, no estructurados.

---

## 2. Cambiar la M√©trica de CV
- **Consigna:** Probar `mean_absolute_error` en lugar de `MSE` en `RidgeCV` y `LassoCV`.
- **Notas te√≥ricas (IMPORTANTE en markdown):**
  - **OLS:** minimiza errores cuadrados ‚Üí sin regularizaci√≥n.
  - **Ridge (L2):** penaliza magnitud de coeficientes, evita sobreajuste.
  - **Lasso (L1):** fuerza algunos coeficientes a cero ‚Üí selecci√≥n de variables.
  - **RidgeCV/LassoCV:** usan CV para elegir Œ± autom√°ticamente.
- **Conclusi√≥n del c√≥digo:**
  - Los valores de Œ± cambian seg√∫n la m√©trica.
  - **MAE** da m√°s peso a errores medios, **MSE** castiga m√°s los errores grandes.
- **Idea clave:** La m√©trica de evaluaci√≥n condiciona la elecci√≥n de hiperpar√°metros y la ‚Äúforma‚Äù del modelo.

---

## 3. Implementar K-Fold a Mano
- **Consigna:** Reproducir lo que hace `RidgeCV` usando `KFold(n_splits=5)` manualmente.
- **Notas (IMPORTANTE):**
  - KFold divide los datos en 5 pliegues.
  - Cada observaci√≥n se usa tanto en train como en validaci√≥n, pero nunca simult√°neamente.
- **Conclusi√≥n del c√≥digo:**
  - El `MSE` promedio manual ‚âà resultado de `RidgeCV`.
  - Confirma que CV robusto entrega resultados estables.
- **Idea clave:** KFold da una estimaci√≥n m√°s confiable que un √∫nico split.

---

## 4. Caracter√≠sticas Polin√≥micas
- **Consigna:** Agregar `GrLivArea¬≤` y comparar R¬≤.
- **Conclusi√≥n del c√≥digo:**
  - El R¬≤ mejora ligeramente al agregar el t√©rmino cuadr√°tico.
  - Muestra que capturar relaciones no lineales (aunque simples) puede aumentar el poder explicativo.
- **Idea clave:** El *feature engineering* polin√≥mico es √∫til cuando la relaci√≥n no es estrictamente lineal.

---

## 5. Impacto de Outliers
- **Consigna:** Quitar la casa m√°s cara y reentrenar OLS.
- **Conclusi√≥n del c√≥digo:**
  - R¬≤ mejora (ej: de 0.8346 ‚Üí 0.8357).
  - Los coeficientes cambian, mostrando que los outliers distorsionaban el ajuste.
- **Idea clave:** Los outliers tienen efecto desproporcionado en regresi√≥n lineal ‚Üí conviene detectarlos.

---

## 6. Comparar MAE vs MSE
- **Consigna:** Calcular MAE en OLS, RidgeCV y LassoCV.
- **Notas te√≥ricas (IMPORTANTE):**
  - **MAE:** promedio de errores absolutos ‚Üí m√°s interpretable.
  - **MSE:** errores al cuadrado ‚Üí castiga fuertemente los grandes errores.
- **Conclusi√≥n del c√≥digo:**
  - Siempre MAE < MSE porque el cuadrado amplifica los errores.
- **Idea clave:** La m√©trica elegida refleja qu√© tipo de error se quiere minimizar.

---

## 7. Importancia de Caracter√≠sticas
- **Consigna:** Ordenar coeficientes de RidgeCV en valor absoluto.
- **Conclusi√≥n del c√≥digo:**
  - Identifica las 3 variables m√°s influyentes en el precio.
- **Idea clave:** Ridge no elimina variables, pero s√≠ revela cu√°les dominan el modelo.

---

## 8. Efecto del Alpha en Lasso
- **Consigna:** Graficar coeficientes vs Œ± con `lasso_path`.
- **Conclusi√≥n visual:**
  - Al aumentar Œ±, los coeficientes se reducen hasta volverse cero.
  - Se observa la **selecci√≥n de variables** en acci√≥n.
- **Idea clave:** Lasso controla complejidad ‚Üí a mayor Œ±, modelo m√°s simple.

---

## 9. Regresi√≥n Lineal Simple vs M√∫ltiple
- **Consigna:** Comparar coeficientes de modelos simples vs OLS m√∫ltiple.
- **Notas (IMPORTANTE):**
  - En regresi√≥n simple, el coef mide el efecto bruto de la variable.
  - En m√∫ltiple, mide el efecto **controlando por las otras**.
- **Conclusi√≥n del c√≥digo:**
  - Los coeficientes difieren mucho cuando hay correlaci√≥n entre variables (multicolinealidad).
- **Idea clave:** La interpretaci√≥n depende de si analizamos una variable sola o en conjunto.

---

## 10. Modelo para Producci√≥n
- **Pregunta:** ¬øQu√© modelo elegir√≠as para producci√≥n?
- **Respuesta (IMPORTANTE en markdown):** **LassoCV**.
- **Justificaci√≥n:**
  - R¬≤ y MSE similares a Ridge.
  - Selecci√≥n autom√°tica de variables ‚Üí interpretabilidad.
  - Simplicidad y menor riesgo de sobreajuste.
- **Idea clave:** En producci√≥n, no siempre se busca el mejor R¬≤, sino un balance entre precisi√≥n, interpretabilidad y simplicidad.

---

# üìå S√≠ntesis General (puntos 1‚Äì10)
- Se cubren conceptos de **validaci√≥n cruzada, m√©tricas, regularizaci√≥n, outliers, polinomios y selecci√≥n de modelo**.
- El hilo conductor es entender que **las decisiones t√©cnicas (m√©trica, Œ±, outliers, features) alteran fuertemente el modelo final**.
- Conclusi√≥n global: **LassoCV es la mejor elecci√≥n en este caso** porque equilibra desempe√±o, interpretabilidad y simplicidad.
