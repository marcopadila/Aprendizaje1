
# Taller Pr√°ctico ‚Äì Ejercicios Adicionales de Introducci√≥n a Machine Learning

Este documento resume y enriquece los **10 ejercicios adicionales** del taller de Machine Learning con el dataset **Iris**.  
Incluye explicaciones, notas importantes y conclusiones, adem√°s de conceptos complementarios para estudiantes que inician en Aprendizaje Autom√°tico.

---

## 1. Estad√≠sticas Descriptivas

**Consigna:** Calcular media, mediana y desviaci√≥n est√°ndar de `sepal length (cm)`.

**Conceptos importantes:**
- **Media:** tendencia central, sensible a valores extremos.
- **Mediana:** valor central, robusta a outliers.
- **Desviaci√≥n est√°ndar:** mide la dispersi√≥n respecto a la media.

**Conclusi√≥n:**  
En Iris, las medidas son consistentes y reflejan un dataset limpio y balanceado.

---

## 2. Filtrado y Agrupaci√≥n

**Consigna:** Filtrar solo flores *virginica* y calcular el promedio de `petal width (cm)`.

**Conceptos importantes:**
- El filtrado permite comparar subconjuntos.
- El agrupamiento (`groupby`) organiza datos por categor√≠as.

**Conclusi√≥n:**  
*Virginica* tiene p√©talos m√°s anchos ‚Üí variable discriminante clave para modelos.

---

## 3. Creaci√≥n de Histogramas

**Consigna:** Graficar histograma de `sepal width (cm)` y variar `bins`.

**IMPORTANTE: ¬øQu√© es un histograma?**
- Representa la distribuci√≥n de una variable num√©rica.
- El eje X son intervalos (bins), el eje Y la frecuencia.
- Muestra patrones de normalidad, sesgo o concentraci√≥n.

**Ejemplo visual:**  
![Histograma Ejemplo](https://upload.wikimedia.org/wikipedia/commons/1/1a/Histogram_example.svg)

**Conclusi√≥n:**  
En Iris, `sepal width` es casi normal. Cambiar `bins` altera la percepci√≥n de la distribuci√≥n.

---

## 4. Visualizaci√≥n con Scatter Plot

**Consigna:** Scatter plot de `sepal length` vs `sepal width`, coloreado por especie.

**Conceptos importantes:**
- Cada punto = un registro (X=variable1, Y=variable2).
- Detecta agrupamientos, correlaciones y outliers.

**Ejemplo visual:**  
![Scatter Plot Ejemplo](https://upload.wikimedia.org/wikipedia/commons/0/02/Iris_dataset_scatterplot.svg)

**Conclusi√≥n:**  
*Setosa* se distingue f√°cilmente; *Versicolor* y *Virginica* se solapan ‚Üí mayor dificultad de clasificaci√≥n.

---

## 5. B√∫squeda por Condici√≥n

**Consigna:** Contar flores con `petal length > 6.0` y crear un DataFrame con ellas.

**Conceptos importantes:**
- Filtrado l√≥gico (`df[df["col"] > valor]`).
- √ötil para detectar valores extremos.

**Conclusi√≥n:**  
Los casos con `petal length > 6` son mayormente *Virginica*, confirmando su dominio en valores grandes.

---

## 6. Re-divisi√≥n de Datos

**Consigna:** `train_test_split` con `test_size=0.33`, `random_state=100`.

**Conceptos importantes:**
- **Test size:** porcentaje de datos para evaluar.  
- **Random state:** asegura reproducibilidad.

**Conclusi√≥n:**  
Un 33% en test implica menos datos para entrenar. La reproducibilidad es fundamental en experimentos.

---

## 7. Ingenier√≠a de Caracter√≠sticas Simple

**Consigna:** Crear columna `petal_area = petal length * petal width` y calcular media.

**Conceptos importantes:**
- **Feature engineering:** creaci√≥n de nuevas variables a partir de las existentes.
- Aporta m√°s poder predictivo a los modelos.

**Conclusi√≥n:**  
El √°rea del p√©talo es m√°s discriminante que cada dimensi√≥n por separado.

---

## 8. An√°lisis de Correlaci√≥n

**Consigna:** Calcular matriz de correlaci√≥n de las 4 caracter√≠sticas y detectar la m√°s alta.

**Conceptos importantes:**
- La correlaci√≥n mide la relaci√≥n entre variables (-1 a 1).
- Cercano a 1 = relaci√≥n positiva fuerte.

**Conclusi√≥n:**  
`petal length` y `petal width` tienen correlaci√≥n ‚âà 0.96 ‚Üí transmiten informaci√≥n muy similar.

---

## 9. Efecto de la Estratificaci√≥n

**Consigna:** Hacer `train_test_split` sin `stratify=y` y comparar distribuciones.

**Conceptos importantes:**
- La **estratificaci√≥n** mantiene proporciones de clases en train/test.
- Sin ella, se pueden generar conjuntos desbalanceados.

**Conclusi√≥n:**  
Estratificar es clave para datasets peque√±os o con clases balanceadas ‚Üí evita sesgos de evaluaci√≥n.

---

## 10. Generalizando el Proceso (Dataset Wine)

**Consigna:** Cargar `load_wine` y explorar con `.head()`, `.info()`, `.describe()`.

**Conceptos importantes:**
- El dataset Wine tiene **13 variables** (m√°s complejo que Iris).
- Practicar exploraci√≥n en datasets distintos es esencial para generalizar habilidades.

**Conclusi√≥n:**  
Explorar, visualizar y preparar datos antes de modelar es un paso universal en ML.

---

# üìå S√≠ntesis Final

Estos 10 ejercicios fortalecen los pilares de la ciencia de datos:

1. **Exploraci√≥n:** estad√≠sticas, histogramas, scatter plots.  
2. **Preparaci√≥n:** divisiones train/test, estratificaci√≥n.  
3. **Transformaci√≥n:** filtrados, agregaciones, feature engineering.  
4. **Generalizaci√≥n:** aplicar la misma metodolog√≠a en diferentes datasets.

**Mensaje clave:** *Antes de entrenar un modelo, el paso m√°s importante es conocer y preparar los datos.*
